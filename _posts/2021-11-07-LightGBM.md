---
title:  "[머신러닝] LightGBM"
excerpt: "LightGBM 원리, 하이퍼파라미터, 해당 알고리즘의 장단점을 다룹니다."

categories:
  - machinelearning
tags:
  - [machinelearning, LightGBM,부스팅, 앙상블]

toc: true
toc_sticky: true
 
date: 2021-11-07
last_modified_at: 2021-11-07


---
# LightGBM

## 1. LightGBM 개요

- LightGBM은 XGboost와 함께 부스팅 계열 알고리즘에서 가장 각광을 받고 있음
- Xgboost는 뛰어난 알고리즘이지만 학습시간이 여전히 오래 걸림, 특히 GridSearchCV로 하이퍼 파라미터 튜닝을 수행하다 보면 수행 시간이 너무 오래 걸려 많은 파라미터를 튜닝하기에 어려움을 겪음, GBM보다는 빠르지만, 대용량 데이터의 경우 만족할만한 학습 성능을 기대하라면 많은 CPU 코어를 가진 시스템에서 높은 병렬도로 학습을 진행해야 함, 가능하지 않으면 불편함이 커짐
- 일반 GBM 계열의 트리 분할 방법과는 다르게 리프 중심 트리 분할(Leaf Wise) 반식을 사용함
- 기존의 대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할(Level Wise) 방식을 사용함. 즉, 최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있습니다. 이렇게 균형 잡힌 트리를 생성하는 이유는 오버피팅에 보다 더 강한 구조를 가질 수 있다고 알려져 있기 때문임, 반대로 균형을 맞추기 위해 시간이 상대적으로 더 필요함
- Leaf Wise 분할 방식은 트리의 균형을 맞추지 않고, 최대 손실값(max delta loss)을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성됨, 하지만 이렇게 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다는 개념임

## 2. LightGBM의 장점

1) XGboost보다 빠른 학습과 예측 수행 시간

2) XGboost보다 메모리 사용량도 상대적으로 적음

3) Xgboost와 LightGBM의 예측 성능은 별다른 차이가 없음

4) 기능상의 다양성이 Xgboost보다 많아짐

5) 카테고리형 피처의 자동 변환과 최적 분할(원핫 인코딩 등을 사용하지 않고도 카테고리형 피처를 최적으로 변환하고 이에 따른 노드 분할 수행)

6) Xgboost와 마찬가지로 대용량 데이터에 대한 뛰어난 예측 성능 및 병렬 컴퓨팅 기능을 제공하며, 최근에는 GPU까지 지원

## 3. LightGBM의 단점

- 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉬움
- 여기서 적은 데이터 세트는 만건 이하의 데이터 세트라고 공식문서에 기술되어 있음

## 4. LightGBM 하이퍼파라미터

- Xgboost와 유사하지만 주의해야 할 점은 LightGBM은 Xgboost와 다르게 리프 노드가 계속 분할되면서 트리의 깊이가 깊어지므로 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다는점

### 4-1. 주요 파라미터

[주요 파라미터](https://www.notion.so/a0637796b81b46d89b702e58061667e0)


![이미지 1106003.jpg](/assets/2021-11-07/이미지_1106003.jpg)
    


### 4-2. Learning Task 파라미터

[Learning Task 파라미터](https://www.notion.so/788f69d9a8f8476cb1c28002706981d7)


![이미지 1106004.jpg](/assets/2021-11-07/이미지_1106004.jpg)
    

### 4-3. 하이퍼파라미터 튜닝 방안

- num_leaves의 갯수를 중심으로 min_child_samples(min_data_in_leaf), max_depth를 함께 조정하면서 모델의 복잡도를 줄이는 것이 기본 튜닝 방안임
- learning_rate를 작게 하면서 n_estimators를 크게 하는 것은 부스팅 계열 튜닝에서 기본적인 튜닝 방안이므로 이를 적용
- 과적합을 제어하기 위해 reg_lambda, reg_alpha와 같은 regularization을 적용하거나 학습 데이터에 사용할 피처의 갯수나 데이터 샘플링 레코드 개수를 줄이기 위해 colsample_bytree, subsample 파라미터를 적용할 수 있음

1) num_leaves: 개별 트리가 가질 수 있는 최대 리프의 갯수이고 LGBM 모델의 복잡도를 제어하는 주요 파라미터임, num leaves 갯수를 높이면 정확도가 높아지지만, 트리 깊이가 깊어지고 모델 복잡도가 커져 과적합 영향도가 커짐

2) min_data_in_leaf(=min_child_sample): 과적합을 개선하기 위한 중요한 파라미터임, num_leaves와 학습 데이터의 크기에 따라 달라지지만 보통 큰 값으로 설정하면 트리가 깊어지는 것을 방지함

3) max_depth는 명시적으로 깊이의 크기를 제한함, num_leaves, min_data_in_leaf와 결합해 과적합을 개선하는 데 사용함
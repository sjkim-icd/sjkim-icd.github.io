---
title: "[Hugging Face][C-2] Behind the pipeline"
header:
  overlay_filter: 0.5

categories:
  - NLP
tags:
  - [Hugging Behind the pipeline"]
comments: true
toc: true
toc_sticky: true
 
date: 2023-09-17
last_modified_at: 2023-09-17
---


# What happens inside the Pipeline Fuction? with Sylvain

![ì´ë¯¸ì§€ 0904001.jpg](/assets/HF/ì´ë¯¸ì§€ 0904001.jpg)

- íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ ë‚´ë¶€ì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€
- Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ë–„ ì‹¤ì œë¡œ ì–´ë–¤ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ ì‚´í´ë³´ê³ ì í•¨
    
    ![ì´ë¯¸ì§€ 0904002.jpg](/assets/HF/ì´ë¯¸ì§€ 0904002.jpg)
    

- sentimental analysisì—ì„œ ë‹¤ìŒ ë‘ ë¬¸ì¥ì—ì„œ ê°ê°ì˜ ì ìˆ˜ì™€ í•¨ê»˜ ê¸ì •ì ì¸ ë¼ë²¨ë¡œ ì–´ë–»ê²Œ ì´ë™í–ˆëŠ”ì§€ ì‚´í´ë³´ê³ ì í•¨

![ì´ë¯¸ì§€ 0904003.jpg](/assets/HF/ì´ë¯¸ì§€ 0904003.jpg)

- íŒŒì´í”„ë¼ì¸ í”„ë ˆì  í…Œì´ì…˜ì—ì„œ ì‚´í´ë³¸ ê²ƒì²˜ëŸ¼ íŒŒì´í”„ë¼ì¸ì—ëŠ” ì„¸ ê°€ì§€ ë‹¨ê³„ê°€ ìˆìŒ
    
    ![ì´ë¯¸ì§€ 0904005.jpg](/assets/HF/ì´ë¯¸ì§€ 0904005.jpg)
    

1) ë¨¼ì € í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ raw textë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•¨

2) ê·¸ëŸ° ë‹¤ìŒ í•´ë‹¹ ìˆ«ìëŠ” ëª¨ë¸ì„ í†µê³¼í•˜ì—¬ logitì„ ì¶œë ¥í•¨

3) ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬í›„ ì²˜ë¦¬ ë‹¨ê³„ì—ì„œëŠ” í•´ë‹¹ logitì„ ë¼ë²¨ê³¼ ì ìˆ˜ë¡œ ë³€í™˜í•¨

- ê° ë‹¨ê³„ë¥¼ ìì„¸íˆ ì‚´í´ë³´ê³ ì í•¨
- ì²« ë²ˆì§¸ ë‹¨ê³„ì¸ í† í°í™”ë¶€í„° ì‹œì‘í•˜ì—¬ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ë³µì œí•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê³ ì í•¨

## 1. Tokenizer

![ì´ë¯¸ì§€ 0904006.jpg](/assets/HF/ì´ë¯¸ì§€ 0904006.jpg)

- Tokenizer í”„ë¡œì„¸ìŠ¤ì—ëŠ” ì—¬ëŸ¬ ë‹¨ê³„ê°€ ìˆìŒ
    
    ![ì´ë¯¸ì§€ 0904008.jpg](/assets/HF/ì´ë¯¸ì§€ 0904008.jpg)
    

**1) ë¨¼ì €, textëŠ” tokenì´ë¼ëŠ” ì‘ì€ ë©ì–´ë¦¬ë¡œ ë¶„í• ë¨**

ë‹¨ì–´, ë‹¨ì–´ì˜ ì¼ë¶€ ë˜ëŠ” êµ¬ë‘ì  ê¸°í˜¸ì¼ ìˆ˜ ìˆìŒ

**2) Tokenizer** **ì—ëŠ” ëª‡ ê°€ì§€ special tokensì´ ìˆìŒ**

ëª¨ë¸ì€ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ì— CLS í† í°ì´ ìˆê³ , 
ë¬¸ì¥ ë ë¶€ë¶„ì— SEP í† í°ì´ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•¨

3) ë§ˆì§€ë§‰ìœ¼ë¡œ TokenizerëŠ” ê° í† í°ì„ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ vocabularyì— ìˆëŠ” unique IDì™€ ë§¤ì¹­ì‹œí‚´

ì´ëŸ¬í•œ Tokenizerë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•´ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” AutoTokenizer APIë¥¼ ì œê³µí•¨

### Tokenizer ì½”ë“œ

![ì´ë¯¸ì§€ 0904009.jpg](/assets/HF/ì´ë¯¸ì§€ 0904009.jpg)

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

- ì´ í´ë˜ìŠ¤ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ methodëŠ” from_pretrainedë¡œ ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ì™€ ê´€ë ¨ëœ êµ¬ì„± ë° ì–´íœ˜ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•¨
- ì—¬ê¸°ì„œ sentiment analysis íŒŒì´í”„ë¼ì¸ì— ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ëŠ” distillbert base uncased Fintuned sst2 englishì„
- í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì™€ ì—°ê²°ëœ Tokenizerë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•œ ë‹¤ìŒ 2ë¬´ì¥ì„ ì œê³µí•¨

1) padding =  True: 

- ë‘ ë¬¸ì¥ì˜ í¬ê¸°ê°€ ë™ì¼í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°°ì—´ì„ ë§Œë“¤ë ¤ë©´ ê°€ì¥ ì§§ì€ ë¬¸ì¥ì„ ì±„ì›€

2) truncation = True: 

- ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ ì˜ë¦¼

3) return_tensors="pt" : 

- return_tensors="pt"
- Tokenizerì—ê²Œ Pytorch í…ì„œë¥¼ ë°˜í™˜í•˜ë„ë¡ í•¨

![ì´ë¯¸ì§€ 0904010.jpg](/assets/HF/ì´ë¯¸ì§€ 0904010.jpg)

![ì´ë¯¸ì§€ 0904011.jpg](/assets/HF/ì´ë¯¸ì§€ 0904011.jpg)

- ê²°ê³¼ë¥¼ ë³´ë©´ 2ê°œì˜ keyê°€ ìˆëŠ” dictionaryê°€ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ
- ì…ë ¥ IDì—ëŠ” 2 ë¬¸ì¥ì˜ IDê°€ ëª¨ë‘ í¬í•¨ë˜ë©° íŒ¨ë”©ì´ ì ìš©ëœ ìœ„ì¹˜ëŠ” 0ì„
- ë‘ ë²ˆì§¸ í‚¤ì¸ attenion maskëŠ” íŒ¨ë”©ì´ ì ìš©ëœ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë¯€ë¡œ ëª¨ë¸ì€ ì´ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì§€ ì•ŠìŒ

## 2. Model

![ì´ë¯¸ì§€ 0904012.jpg](/assets/HF/ì´ë¯¸ì§€ 0904012.jpg)

- 2 ë²ˆì§¸ ë‹¨ê³„ì¸ ëª¨ë¸ì„ ì‚´í´ë³´ê³ ì í•¨
- í† í¬ë‚˜ì´ì €ì˜ ê²½ìš°, from_pretrained ë©”ì„œë“œê°€ í¬í•¨ëœ **AutoModel API**ê°€ ìˆìŒ
- ëª¨ë¸ì˜ êµ¬ì„±ê³¼ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•¨
    
    ![ì´ë¯¸ì§€ 0904013.jpg](/assets/HF/ì´ë¯¸ì§€ 0904013.jpg)
    

![ì´ë¯¸ì§€ 0904014.jpg](/assets/HF/ì´ë¯¸ì§€ 0904014.jpg)

- **AutoModel APIëŠ” ëª¨ë¸ì˜ ë³¸ë¬¸, ì¦‰ pretraining headê°€ ì œê±°ëœ í›„ ë‚¨ì€ ëª¨ë¸ ë¶€ë¶„ë§Œ ì¸ìŠ¤í„´ìŠ¤í™”í•¨**
- ì „ë‹¬ëœ ë¬¸ì¥ì„ í‘œí˜„í•˜ì§€ë§Œ ë¶„ë¥˜ ë¬¸ì œì— ì§ì ‘ì ìœ¼ë¡œ ìœ ìš©í•˜ì§€ëŠ” ì•Šì€ ê³ ì°¨ì› í…ì„œë¥¼ ì¶œë ¥í•¨
- ì—¬ê¸°ì„œ í…ì„œëŠ” ë‘ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ê° í† í°ì€ 16ê°œì´ë©° ë§ˆì§€ë§‰ ì°¨ì›ì€ ëª¨ë¸ 768ì˜ hidden sizeì„
    
    ![ì´ë¯¸ì§€ 0904015.jpg](/assets/HF/ì´ë¯¸ì§€ 0904015.jpg)
    
- ë¶„ë¥˜ë¬¸ì œì™€ ì—°ê²°ëœ ì¶œë ¥ì„ ì–»ìœ¼ë ¤ë©´ AutoModelForSequenceClassification í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼í•¨
- classification headê°€ ìˆëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ AutoModel í´ë˜ìŠ¤ì™€ ë™ì¼í•˜ê²Œ ì‘ë™í•¨
- Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” ì¼ë°˜ì ì¸ NLP taskë§ˆë‹¤ í•˜ë‚˜ì˜ auto classê°€ ìˆìŒ
    
    ![ì´ë¯¸ì§€ 0904016.jpg](/assets/HF/ì´ë¯¸ì§€ 0904016.jpg)
    
- ì—¬ê¸°ì„œëŠ” ëª¨ë¸ì— 2ê°œì˜ ë¬¸ì¥ì„ ì œê³µí•œ í›„ 2X2 í¬ê¸°ì˜ í…ì„œë¥¼ ì–»ìŒ
- ê° ë¬¸ì¥ê³¼ possible labelì´ ìƒì„±ë¨
- ì´ëŸ¬í•œ ì¶œë ¥ì€ ì•„ì§ í™•ë¥ ì€ ì•„ë‹˜(í•©ê³„ê°€ 1ì´ ì•„ë‹˜)
- ì´ëŠ” Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê° ëª¨ë¸ì´ ë¡œê·¸ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì„

## 3. Preprocessing

![ì´ë¯¸ì§€ 0905002.jpg](/assets/HF/ì´ë¯¸ì§€ 0905002.jpg)

- íŒŒì´í”„ë¼ì¸ì˜ ì„¸ë²ˆì§¸ ì´ì ë§ˆì§€ë§‰ ë‹¨ê³„ì¸ preprocessingì„ ì‚´í´ë´ì•¼ í•¨
- logitì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ë ¤ë©´ softmax ë ˆì´ì–´ë¥¼ ì ìš© í•´ì•¼ í•¨
    
    ![ì´ë¯¸ì§€ 0905003.jpg](/assets/HF/ì´ë¯¸ì§€ 0905003.jpg)
    
- í•©ì´ 1ì´ ë˜ëŠ” ì–‘ìˆ˜ë¡œ ë³€í™˜ë¨
    
    ![ì´ë¯¸ì§€ 0905004.jpg](/assets/HF/ì´ë¯¸ì§€ 0905004.jpg)
    
- ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ì–‘ìˆ˜ ë˜ëŠ” ìŒìˆ˜ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ”ì§€ ì•„ëŠ” ë¶€ë¶„ì„
- ì´ëŠ” ëª¨ë¸ êµ¬ì„±ì˜ id2label í•„ë“œë¥¼ í†µí•´ì„œ ì œê³µë¨
- ì²«ë²ˆì§¸ í™•ë¥ (index 0)ì€ ìŒìˆ˜ì´ê³ , index 1ì€ ì–‘ìˆ˜ ë¼ë²¨ì— í•´ë‹¹ë¨
- ì´ê²ƒì´ íŒŒì´í”„ë¼ì¸ ê¸°ëŠ¥ìœ¼ë¡œ êµ¬ì¶•ëœ ë¶„ë¥˜ê¸°ê°€ í•´ë‹¹ ë¼ë²¨ì„ ì„ íƒí•˜ê³  í•´ë‹¹ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„
    
    ![ì´ë¯¸ì§€ 0905005.jpg](/assets/HF/ì´ë¯¸ì§€ 0905005.jpg)
    
    ![ì´ë¯¸ì§€ 0905007.jpg](/assets/HF/ì´ë¯¸ì§€ 0905007.jpg)
    
- ì´ì œ ê° ë‹¨ê³„ì˜ ì‘ë™ ë°©ì‹ì„ ì•Œì•˜ìœ¼ë¯€ë¡œ í•„ìš”ì— ë§ê²Œ ì‰½ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŒ

# ****Behind the pipeline****

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

# result

[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

- 
- íŒŒì´í”„ë¼ì¸ì€ ì „ì²˜ë¦¬(preprocessing), ëª¨ë¸ë¡œ ì…ë ¥ ì „ë‹¬ ë° í›„ì²˜ë¦¬(postprocessing)ì˜ 3ë‹¨ê³„ë¥¼ í•œë²ˆì— ì‹¤í–‰í•¨
    
    ![ì´ë¯¸ì§€ 0905008.jpg](/assets/HF/ì´ë¯¸ì§€ 0905008.jpg)
    

## [Preprocessing with a tokenizer](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt#preprocessing-with-a-tokenizer)

- ë‹¤ë¥¸ neural networksê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Transformer ëª¨ë¸ì€ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ íŒŒì´í”„ë¼ì¸ì˜ ì²«ë²ˆì§¸ ë‹¨ê³„ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•¨
- ì´ë¥¼ ìœ„í•´, ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ ìˆ˜í–‰í•˜ëŠ” tokenizerë¥¼ ì‚¬ìš©í•¨
    
    **1) Splitting the input into words, subwords, or symbols (like punctuation) that are calledÂ *tokens***
    
    - ì…ë ¥ì„ words, subwords, symbolsë¡œ splitting
    
    **2) Mapping each token to an integer**
    
    - ê° í† í°(token)ì„ ì •ìˆ˜(integer)ë¡œ ë§¤í•‘(mapping)
    
    **3) Adding additional inputs that may be useful to the model**
    
    - ëª¨ë¸ì— ìœ ìš©í•œ additional inputs ì¶”ê°€
- ì´ ëª¨ë“  preprocessingëŠ” ëª¨ë¸ì´ pretrainingë  ë•Œì™€ ì •í™•íˆ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë¨¼ì €Â [Model Hub](https://huggingface.co/models)ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ë‹¤ìš´ë¡œë“œì•¼ í•¨
- ì´ë¥¼ ìœ„í•´Â `AutoTokenizer`Â í´ë˜ìŠ¤ì™€Â `from_pretrained()`Â ë©”ì„œë“œë¥¼ ì‚¬ìš©í•¨
- ëª¨ë¸ì˜ checkpoint ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ tokenizerì™€ ì—°ê²°ëœ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´
- ê·¸ë˜ì„œ ì•„ë˜ ì½”ë“œë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œë§Œ í•´ë‹¹ ì •ë³´ê°€ ë‹¤ìš´ë¡œë“œë¨
- `sentiment-analysis`Â íŒŒì´í”„ë¼ì¸ì˜default checkpointëŠ”Â `distilbert-base-uncased-finetuned-sst-2-english`([ì—¬ê¸°](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english))ì´ë¯€ë¡œ ë‹¤ìŒì„ ì‹¤í–‰í•¨

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

- ìœ„ì™€ ê°™ì´ tokenizerë¥¼ ìƒì„±í•˜ë©´, ì•„ë˜ì˜ ì½”ë“œì—ì„œ ë³´ëŠ” ê²ƒì²˜ëŸ¼, ì´ tokenizerì— ë¬¸ì¥ì„ ì…ë ¥í•˜ì—¬ ëª¨ë¸ì— ë°”ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” íŒŒì´ì¬ dictionary ì •ë³´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ ì´í›„ í•´ì•¼í•  ì¼ì€ input IDs ë¦¬ìŠ¤íŠ¸ë¥¼ tensorsë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„
- tokenizerê°€ ë°˜í™˜í•˜ëŠ” í…ì„œì˜ ìœ í˜•(PyTorch, TensorFlow ë˜ëŠ” ì¼ë°˜ NumPy)ì„ ì§€ì •í•˜ë ¤ë©´Â `return_tensors`Â ì¸ìˆ˜(argument)ë¥¼ ì‚¬ìš©í•˜ë©´ ë¨

```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

- `padding`ê³¼Â `truncation`ì€ ì´í›„ì— ì„¤ëª… ì˜ˆì •
- ì¤‘ìš”í•œ ì ì€ ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë‹¤ì¤‘ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ tokenizer í•¨ìˆ˜ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆì„ ë¿ë§Œ ì•„ë‹ˆë¼ ì¶œë ¥ í…ì„œ ìœ í˜•ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ë¶€ë¶„ì„
- í…ì„œ ìœ í˜•ì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ê²°ê³¼ë¡œ ì´ì¤‘ ë¦¬ìŠ¤íŠ¸(list of list)ê°€ í‘œì‹œë¨
- PyTorch í…ì„œ ìœ í˜•ì˜ ê²°ê³¼ëŠ” ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. ìœ„ ê²°ê³¼ì—ì„œ ë³´ë“¯ì´, ì¶œë ¥ì€ ë‘ ê°œì˜ í‚¤(key) ì¦‰,Â `input_ids`Â ë°Â `attention_mask`ë¥¼ ê°€ì§€ëŠ” íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ì„
- `input_ids`ì—ëŠ” ê° ë¬¸ì¥ì— ìˆëŠ” í† í°ì˜ ê³ ìœ  ì‹ë³„ìë¡œ êµ¬ì„±ëœ ë‘ í–‰ì˜ ì •ìˆ˜(ê° ë¬¸ì¥ì— í•˜ë‚˜ì”©)ê°€ ê°’(value)ìœ¼ë¡œ ë“¤ì–´ê°€ ìˆìŒ
- í•´ë‹¹ ì¥ ë’· ë¶€ë¶„ì— Â `attention_mask`Â ì„¤ëª… ì˜ˆì •

## ****Going through the model****

- tokenizerì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ pretrained modelì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŒ
- TransformersëŠ” ìœ„ì˜Â `AutoTokenizer`Â í´ë˜ìŠ¤ì™€ ë§ˆì°¬ê°€ì§€ë¡œ,Â `from_pretrained()`Â ë©”ì„œë“œê°€ í¬í•¨ëœÂ `AutoModel`Â í´ë˜ìŠ¤ë¥¼ ì œê³µí•¨

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

- ìœ„ code snippetì—ì„œëŠ” ì´ì „ì— íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”(instantiate)í•¨
- í•´ë‹¹ ì•„í‚¤í…ì²˜ì—ëŠ” ê¸°ë³¸ Transformer ëª¨ë“ˆë§Œ í¬í•¨ë˜ì–´ ìˆìŒ
- ë”°ë¼ì„œ, ì…ë ¥ì´ ì£¼ì–´ì§€ë©´Â *featureë¼*ê³ ë„ ë¶ˆë¦¬ëŠ”Â *hidden states*Â ë¥¼ ì¶œë ¥í•¨
- ê° ëª¨ë¸ ì…ë ¥ì— ëŒ€í•´Â **Transformer ëª¨ë¸ì— ì˜í•´ì„œ ìˆ˜í–‰ëœ í•´ë‹¹ ì…ë ¥ì˜ ë¬¸ë§¥ì  ì´í•´(contextual understanding) ê²°ê³¼**Â ë¥¼ ë‚˜íƒ€ë‚´ëŠ” high-dimensional vectorë¥¼ ê°€ì ¸ì˜´
- hidden statesëŠ” ê·¸ ìì²´ë¡œë„ ìœ ìš©í•  ìˆ˜ ìˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œÂ *head*Â ë¼ê³  ì•Œë ¤ì§„ ëª¨ë¸ì˜Â *ë‹¤ë¥¸*Â ë¶€ë¶„ì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°
- 1ì¥ì—ì„œ ì´ì•¼ê¸°í–ˆë“¯ì´, ê°™ì€ ì•„í‚¤í…ì²˜ë¡œ ì„œë¡œ ë‹¤ë¥¸ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ ê° taskì— ëŒ€í•´ì„œëŠ” ë‹¤ë¥¸ headê°€ ì—°ê²°ë˜ì–´ ìˆìŒ.

### ****A high-dimensional vector?****

- Transformer ëª¨ë“ˆì˜ ë²¡í„° ì¶œë ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ê·œëª¨ê°€ í½ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì„¸ ê°€ì§€ ì°¨ì›ì´ ìˆìŒ
    
    **1) Batch size:** 
    - í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” ì‹œí€€ìŠ¤(sequence)ì˜ ê°œìˆ˜
    - ìœ„ ì˜ˆì œ 2ê°œ
    
    **2) Sequence length:**
    - ì‹œí€€ìŠ¤ ìˆ«ì í‘œí˜„ì˜ ê¸¸ì´
    - ìœ„ ì˜ˆì œ 16
    **3) ì€ë‹‰ í¬ê¸°(Hidden size):**
    - ê° ëª¨ë¸ ì…ë ¥ì˜ ë²¡í„° ì°¨ì›.
    
- ìœ„ì—ì„œ ë§ˆì§€ë§‰ ê°’ ë•Œë¬¸ì— high-dimensional ë²¡í„°ë¼ê³  ë¶€ë¦„
- Hidden sizeëŠ” ë§¤ìš° í´ ìˆ˜ ìˆìŒ(768ì€ ì‘ì€ ëª¨ë¸ì— ì¼ë°˜ì ì´ê³  í° ëª¨ë¸ì—ì„œëŠ” 3072 ì´ìƒ).
- ì‚¬ì „ ì²˜ë¦¬í•œ ì…ë ¥ì„ ëª¨ë¸ì— ë„˜ê¸°ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆìŒ

```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

- Transformers ëª¨ë¸ì˜ ì¶œë ¥ì€Â `namedtuple`Â ë˜ëŠ” dictionaryì²˜ëŸ¼ ë™ì‘í•¨.
- ìš”ì†Œì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œ ì†ì„± ë˜ëŠ” í‚¤(`outputs["last_hidden_state"]`)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
- ë˜í•œ, ì°¾ê³  ìˆëŠ” í•­ëª©ì´ ì–´ë””ì— ìˆëŠ”ì§€ ì •í™•íˆ ì•Œê³  ìˆëŠ” ê²½ìš° ì¸ë±ìŠ¤(`outputs[0]`)ë¡œë„ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŒ

### ****Model heads: Making sense out of numbers****

- model headëŠ” hidden statesì˜ high-dimensional vectorë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹¤ë¥¸ ì°¨ì›ì— projectí•¨
- ì¼ë°˜ì ìœ¼ë¡œ headëŠ” í•˜ë‚˜ ë˜ëŠ” ëª‡ ê°œì˜ linear layersë¡œ êµ¬ì„±ë¨

![ì´ë¯¸ì§€ 0905009.jpg](/assets/HF/ì´ë¯¸ì§€ 0905009.jpg)

- Transformer ëª¨ë¸ì˜ ì¶œë ¥ì€ ì²˜ë¦¬í•  model headë¡œ ì§ì ‘ ì „ë‹¬ë¨
- ìœ„ ê·¸ë¦¼ì—ì„œ ëª¨ë¸ì€ embeddings layerì™€ subsequent layersë¡œ í‘œí˜„ë¨
- embeddings layerëŠ” tokenized inputì˜ ê° ì…ë ¥ IDë¥¼ í•´ë‹¹ í† í°ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°(embeddings vector)ë¡œ ë³€í™˜í•¨
- ê·¸ ì´í›„ì˜ í›„ì† ë ˆì´ì–´ëŠ” attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ì´ë“¤ embeddings vectorë¥¼ ì¡°ì‘í•˜ì—¬ ë¬¸ì¥ì˜ final representationì„ ìƒì„±í•¨
- Transformersì—ëŠ” ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ê°€ ìˆìœ¼ë©° ê° ì•„í‚¤í…ì²˜ëŠ” íŠ¹í™”ëœ ì‘ì—…ì„ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŒ
- ë‹¤ìŒì€ ì¼ë¶€ ì•„í‚¤í…ì²˜ì„
    - `Model`Â (retrieve the hidden states)
    - `ForCausalLM`
    - `ForMaskedLM`
    - `ForMultipleChoice`
    - `ForQuestionAnswering`
    - `ForSequenceClassification`
    - `ForTokenClassification`
    - and others ğŸ¤—
- ì´ ì„¹ì…˜ì—ì„œì˜ ì˜ˆì‹œì—ì„œëŠ” sequence classification headê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ëª¨ë¸ì´ í•„ìš”í•¨(ë¬¸ì¥ì„ ê¸ì • ë˜ëŠ” ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œ).
- ë”°ë¼ì„œ ì‹¤ì œë¡œÂ `AutoModel`Â í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€ì‹ Â `AutoModelForSequenceClassification`ë¥¼ ì‚¬ìš©í•¨

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

```python
print(outputs.logits.shape)

# result:
torch.Size([2, 2])
```

- ì´ì œ ì¶œë ¥ì˜ shapeì„ ë³´ë©´ ì°¨ì›ì´ í›¨ì”¬ ë‚®ì•„ì§
- model headëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ë‘ ê°œì˜ ê°’(ë ˆì´ë¸”ë‹¹ í•˜ë‚˜ì”©)ì„ í¬í•¨í•˜ëŠ” ë²¡í„°ë¥¼ ì¶œë ¥í•¨
- ë‘ ê°œì˜ ë¬¸ì¥ê³¼ ë‘ ê°œì˜ ë ˆì´ë¸”ë§Œ ìˆê¸° ë•Œë¬¸ì—, ëª¨ë¸ì—ì„œ ì–»ì€ ê²°ê³¼ì˜ ëª¨ì–‘(shape)ì€ 2 x 2ì„

## ****Postprocessing the output****

- ëª¨ë¸ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ì–»ì€ ê°’ì€ ë°˜ë“œì‹œ ê·¸ ìì²´ë¡œ ì˜ë¯¸ê°€ ìˆëŠ” ê²ƒì€ ì•„ë‹˜

```python
print(outputs.logits)

# result
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

- ëª¨ë¸ì€ ì²« ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•´Â `[-1.5607, 1.6123]`, ë‘ ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•´Â `[4.1692, -3.3464]`ë¥¼ ì˜ˆì¸¡í•¨
- ì´ëŠ” í™•ë¥ ì´ ì•„ë‹ˆë¼ ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ê³„ì¸µì—ì„œ ì¶œë ¥ëœ ì •ê·œí™”ë˜ì§€ ì•Šì€ ì ìˆ˜ì¸Â **logits** ì„
- ì´ë“¤ ê°’ì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ë ¤ë©´Â [SoftMax](https://en.wikipedia.org/wiki/Softmax_function)Â ê³„ì¸µì„ í†µê³¼í•´ì•¼ í•¨
- all Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

# ê²°ê³¼
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```

- ì´ì œ ëª¨ë¸ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•´Â `[0.0402, 0.9598]`, ë‘ ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•´Â `[0.9995, 0.0005]`ë¥¼ ì˜ˆì¸¡í–ˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ â†’ í™•ë¥  ì ìˆ˜

```python
model.config.id2label

# result
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

- ê° ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´, model.configì˜Â `id2label`Â ì†ì„±ê°’ì„ í™•ì¸í•¨
- ì´ì œ ëª¨ë¸ì´ ì•„ë˜ ë‚´ìš©ì„ ì˜ˆì¸¡í•¨ì„ ì•Œ ìˆ˜ ìˆìŒ
- ì²«ë²ˆì§¸ ë¬¸ì¥ : NEGATIVE: 0.0402, POSITIVE: 0.9598
- ë‘ë²ˆì§¸ ë¬¸ì¥ : NEGATIVE: 0.9995, POSITIVE: 0.0005

ì§€ê¸ˆê¹Œì§€ íŒŒì´í”„ë¼ì¸(pipeline)ì˜ ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ëŠ” 3ë‹¨ê³„ì¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ ì „ì²˜ë¦¬(preprocessing), ëª¨ë¸ì„ í†µí•œ ì…ë ¥ ì „ë‹¬(passing the inputs through the model) ë° í›„ì²˜ë¦¬(postprocessing)ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰í•´ë´„.
---
title:  "[머신러닝] 결정 트리"
excerpt: "결정 트리"

categories:
  - machinelearning
tags:
  - [machinelearning, 결정 트리, 의사결정나무, decision tree]

toc: true
toc_sticky: true
 
date: 2021-10-09
last_modified_at: 2021-10-09

---
# 결정 트리

# [결정트리]

## 1. 결정 트리의 일반적인 특징

1) 쉽고 유연하게 적용될 수 있는 알고리즘

2) 데이터의 스케일링이나 정규화 등의 사전 가공 영향이 매우 적음

3) 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며, 이로 인한 과적합이 발생해 반대로 예측 성능이 저하될 수 있음

4) 3)번의 단점은 앙상블 기법에서는 장점으로 작용하는데 앙상블의 경우, 여러 약한 학습기를 결합해 확률적으로 보완하고, 오류가 발생한 부분에 대해 가중치를 계속 업데이트하며 예측 성능을 향상시키며, 이때 결정 트리가 좋은 약한 학습기가 됨

## 2. 결정트리의 원리


![이미지 1009008.jpg](/assets/2021-10-09/이미지_1009014.jpg)


- 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘
- 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것
- if/else 기반으로 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘
- 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함
- 결정 트리의 구조
1) 규칙 노드: 규칙 조건이 됨
2) 리프 노드: 결정된 클래스 값
3) 서브 트리: 새로운 규칙 조건마다 생성됨
트리의 깊이가 길어질수록 과적합될 확률이 높아지고 결정 트리의 예측 성능이 저하될 가능성이 높음
- 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 함
- 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듦, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트를 쪼개는 방식으로 트리를 구성함
- 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 1) Information Gain 지수 와 2) 지니 계수가 있음

1) Information Gain : 엔트로피(주어진 데이터 집합의 혼잡도, 서로 다른 값이 섞여있으면 엔트로피가 높음) 개념을 기반으로 하며, 1-엔트로피 지수가 Information gain임, 이 지수가 높은 속성을 기준으로 활용하여 분할 기준을 정함

2) 지니계수: 경제학의 불평등 지수를 나타낼 때 사용하는 계수인데, 0이 가장 평등하고 1로 갈수록 불평등함, 머신러닝에서는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석하여 지니 계수가 낮은 속성을 기준으로 분할함

- Information gain가 높거나 지니 계수가 낮은 조건을 찾아 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정함

## 3. 결정 트리의 특징

1. 결정 트리 룰이 매우 명확하고, 어떻게 규칙 노드와 리프 노드가 만들어지는 지를 알 수 있고, 시각화로 표현할 수 있음

2. 정보의 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 피처 스케일링과 정규화 같은 전처리 작업이 필요 없음

3) 피처 정보의 균일도에 따른 룰 규칙으로 서브 트리를 계속 만들다 보면 피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 커지고 복잡해져 과적합으로 인한 정확도가 떨어짐 

4) 모든 데이터의 케이스를 만족하는 완벽한 규칙은 만들수 없으므로, 트리의 크기를 사전에 제한하는 것이 성능 개선에 도움이 됨

## 4. 결정 트리의 파라미터

**1. min_samples_split** : 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합 제어에 사용됨, 작게 설정할수록 과적합 가능성 증가

**2. min_samples_leaf**: 말단 노드인 leaf가 되기 위한 최소한의 샘플 데이터 수, 과적합 제어에 사용되는데 imbalanced 데이터의 경우 특정 클래스의 데이터가 극히 작을 수 있어 작게 설정이 필요함

**3. max_features**: 최적의 분할을 위해 고려해야 할 최대 피처 수, 디폴트는 None이고 데이터 세트의 모든 피처를 사용해 분할을 수행함

**4. max_depth** : 트리의 최대 깊이를 규정, 디폴트는 None이며 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키움

**5. max_leaf_nodes** : 말단 노드인 leaf의 최대 갯수

## 5. 결정 트리의 과적합

**1) 결정 트리 하이퍼파라미터 디폴트 설정**: iris 데이터를 사용하여 의사결정나무를 만들었을 때 트리 생성에 별도의 제약 없이 결정 트리의 하이퍼 파라미터를 디폴트로 한 경우, 일부 이상치 데이터까지 분류하기 위해 분할이 자주 일어나 결정 기준 경계가 많아짐, 디폴트 설정은 리프 노드 안에 데이터가 모두 균일하거나 하나만 존재해야하는 엄격한 분할 기준으로 인해 복잡해짐

![이미지 1009005.jpg](/assets/2021-10-09/이미지_1009005.jpg)

**2) min_samples_leaf = 6으로 제한 조건을 건 경우** : 이상치에 크게 반응하지 않으며 general한 분류 규칙에 따라 분류됨

![이미지 1009005.jpg](/assets/2021-10-09/이미지_1009006.jpg)

---

reference

파이썬 머신러닝 완벽가이드
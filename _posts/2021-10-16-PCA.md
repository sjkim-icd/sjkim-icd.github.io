---
title:  "[머신러닝] PCA"
excerpt: "PCA"

categories:
  - machinelearning
tags:
  - [machinelearning, PCA]
comments: true
toc: true
toc_sticky: true
 
date: 2021-10-16
last_modified_at: 2021-10-16

---

# PCA

# [PCA]

## 1. 차원축소 개요

### 1-1. **차원 축소의 의미:**

- 차원 축소는 많은 피처로 구성된 다차원의 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것

### 1-2.**차원이 높은 경우의 문제**

1) 일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고, 희소한 구조를 가지게 됨

2) 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어짐

3) 또한, 피처가 많을 경우, 개별 피처 간에 상관관계가 높을 가능성이 큼

⇒ 선형 회귀와 같은 선형 모델에서는 입력 변수 간의 상관관계가 높을 경우 다중 공선성 문제로 모델 예측 성능이 저하됨

### 1-3. **차원 축소를 한 경우 장점**

1. 다차원의 피처를 차원 축소해 피처 수를 줄이면 더 직관적으로 데이터를 해석할 수 있음

-예를 들어 수십 개 이상의 피처의 데이터로 시각적 표현으로 데이터 특성을 파악하기는 어려운데 3차원 이하의 차원 축소를 통해서 시각적으로 데이터를 압축해서 표현할 수 있음

2. 차원 축소 시 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력도 줄일 수 있음

## 2. 차원 축소 종류

### 2-1. 피처 선택

- 피처 선택은 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 자타내는 주요 피처만 선택하는 것

### 2-2. 피처 추출

- 피처 추출은 기존 피처를 저차원의 중요 피처로 압축해 추출하는 것으로, 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 됨
- 기존 피처를 단순 압축하는 것이 아니라 피처를 함축적으로 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것임
- 함축적인 특성 추출은 기존 피처가 인지하기 어려웠던 잠재적인 요소를 추출함
- PCA, SVD, NMF는 잠재적인 요소를 찾는 대표적인 차원 축소 알고리즘임
- ex. 학생 평가 요소로 모의고사 성적, 종합 내신성적, 수상 경력 등 → 학업 성취도, 문제 해결력 같은 함축적인 요약 특성으로 추출할 수 있음

## 3. PCA

### 3-1. PCA의 용도

1. 여러 피처(변수)가 있는 경우, 이들을 PCA 변환하여 변수 갯수를 줄여주는 차원 축소 역할

2. 많은 변수 갯수를 줄여주는 역할 뿐만 아니라 여러 변수의 주요 성분을 추출해 내는 역할을 함

3. 분류, 회귀 예측에서 피처 엔지니어링을 할 때 적용되는데 수 많은 변수들을 PCA로 압축한 후 원본 데이터와 결합하여 추가 변수를 만들어 예측 성능을 향상시킴 

4. 적용되는 분야에 따라 차원 축소, 주성분 도출, 데이터 압축, 노이즈 제거(이미지등)등의 수행함

### 3-2. PCA의 개요

- Principal Component Analysis
- 비지도 학습, 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분(Principal Component)을 추출해 차원을 축소하는 기법
- PCA는 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소하는데, 이것이 PCA의 주성분이 됨(분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주함)
- PCA는 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법
- 2개의 피처를 한 개의 주성분을 가진 데이터 세트로 차원 축소가 가능한데, 데이터 변동성이 가장 큰 방향으로 축을 생성하고, 새롭게 생성된 축으로 데이터를 투영하는 방식임

### 3-3. PCA 원리

- 가장 큰 데이터 변동성(Variance)을 기반으로 첫 번째 벡터 축을 생성하고, 두 번째 축은 이 벡터 축에 직각이 되는 벡터를 축으로 함, 세 번째 축은 다시 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성함, 생성된 벡터 축에 원본 데이터를 투영하면 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소됨
- 선형대수 관점에서는 입력 데이터의 공분산 행렬을 고유값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것임, 이 고유벡터가 PCA의 주성분 벡터로서 입력 데이터의 분산이 큰 방향을 나타냄, 고유값(eigenvalue)은 바로 이 고유벡터의 크기를 나타내며, 동시에 입력 데이터의 분산을 나타냄
- 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며, 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변환하는 방식이 PCA임

### 3-4. PCA STEP

1.입력 데이터 세트의 공분산 행렬을 생성함

2.공분산 행렬의 고유벡터와 고유값을 계산함

3.고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출함

4.고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환함

PCA는 많은 속성으로 구성된 원본 데이터를 핵심으로 구성하는 데이터로 압축함

---

Reference

- 파이썬 머신러닝 완벽가이드
---
title:  "[머신러닝] Xgboost"
excerpt: "Xgboost의 원리, 하이퍼파라미터, 해당 알고리즘의 장단점을 다룹니다."

categories:
  - Machine-Learning
tags:
  - [machinelearning, Xgboost,부스팅, 앙상블]

toc: true
toc_sticky: true
 
date: 2021-11-07
last_modified_at: 2021-11-07


---

 # Xgboost

## 1. Xgboost 개요

- Xgboost는 트리 기반의 앙상블 학습에서 인기 있는 알고리즘
- 캐글대회에서 다수 상위를 차지함
- 분류에 있어서 다른 머신러닝보다 뛰어난 예측 성능을 가짐
- GBM에 기반하고 있으나, GBM의 단점인 느린 수행 시간과 과적합 규제의 부재 등의 문제를 해결하여 각광 받고 있음
- 병렬 CPU 환경에서 병렬학습이 가능해 기존 GBM 보다 빠르게 학습을 완료할 수 있음

## 2. 장단점

### 2-1. 장점

**1) 뛰어난 예측 성능**

 분류와 회귀 영역에서 뛰어난 예측 성능 발휘

**2) GBM 대비 빠른 수행 시간**

- GBM은 순차적으로 Weak learner가 가중치를 증감하는 방법으로 학습하여 속도가 느리지만 Xgboost의 경우, 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행이 가능함, GBM 계열에서 속도가 빠르지만 다른 머신러닝에 비교해 빠른 것은 아님

**3) 과적합 규제**

- 일반 GBM은 과적합 규제 방법이 없으나, Xgboost는 과적합 규제 기능이 있음

**4) Tree pruning**

- 일반 GBM은 분할 시 부정 손실이 발생하여 분할을 더 이상 수행하지 않지만, 이런 경우도 지나치게 많은 분할을 만들 수 있어, max_depth로 분할 깊이를 조정하기도 하지만 tree pruning으로 긍정 이득이 없는 분할을 가지치기 하여 분할 수를 더 줄이기도 함

**5) 내장된 교차 검증**

- Xgboost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행 횟수를 가질 수 있음
- 지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의 평가 값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있음

**6) 결손값 자체 처리**

- Xgboost는 결손값을 자체 처리할 수 있는 기능을 가지고 있음

## 3. 하이퍼 파라미터

- 일반 파라미터, 부스터 파라미터, 학습 태스크 파라미터로 유형이 나뉨

**1) 일반 파라미터**: 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터로서 디폴트 파라미터 값을 바꾸는 경우는 많지 않음

**2) 일반 파라미터**: 트리 최적화, 부스팅, regularization 등과 같은 파라미터 등을 지칭함

**3) 학습 태스크 파라미터** : 학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 파라미터

### 3-1. 일반 파라미터

- 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터로서 디폴트 파라미터 값을 바꾸는 경우는 많지 않음

1) booster : gbtree(tree based model) 또는 gblinear(lineaer model) 선택, 디폴트는 gbtree

2) silent: 디폴트는 0이며, 출력 메시지를 나타내고 싶지 않을 경우 1로 설정

3) nthread: CPU의 실행 스레드 개수를 조정하며, 디폴트는 CPU의 전체 스레드를 다 사용하는 것, 멀티 코어/스레드 CPU시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해 ML 애플리케이션을 구동하는 경우에 변경

### 3-2. 부스터 파라미터

- 트리 최적화, 부스팅, regularization 등과 같은 파라미터 등을 지칭함
  
![이미지 1107001.jpg](/assets/2021-11-07/이미지_1107001.jpg)


### 3-3. 학습 태스크 파라미터

- 학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 파라미터
- 뛰어난 알고리즘일수록 파라미터를 튜닝할 필요가 적음, 파라미터 튜닝에 들이는 공수 대비 성능 향상 효과가 높지 않는 경우가 대부분이라고 함
- 파라미터를 튜닝하는 경우의 수는 여러 가지 상황에 따라 달라짐. 피처의 수가 매우 많거나 피처 간 상관되는 정도가 많거나 데이터 세트에 따라 여러 가지 특성이 있을 수 있음
- 과적합 문제가 심하다면

1) eta 값 낮추기(0.01~0.1) : eta 값을 낮춰줄 경우 num_round(또는 n_estimators)는 반대로 높여줘야 함

2) max_depth 값 낮추기

3) min_child_weight 값 높이기

4) gamma 값 높이기

5) subsample과 colsample_bytree를 조정하는 것도 트리가 너무 복잡하게 생성되는 것을 막아 과적합 문제에 도움이 될 수 있음


 ![이미지 1107002.jpg](/assets/2021-11-07/이미지_1107002.jpg)


## 4. 특징

1) Xgboost 자체적으로 교차 검증, 성능 평가, 피처 중요도 등의 시각화 기능을 가짐

2) 기본 GBM에서 부족한 여러 성능 향상 기능이 있음

- 조기 중단(Early Stopping) : 기존 GBM은 n_estimators에 지정된 횟수만큼 반복적으로 학습 오류를 감소시키며 학습을 진행하면서 중간에 반복을 멈출 수 없고, n_estimators에 지정된 횟수를 다 완료해야하는데 Xgboost와 LGBM은 모두 조기 중단 기능이 있어 부스팅 반복 횟수에 도달하지 않더라도 예측 오류가 더 이상 개선되지 않으면 반복을 끝까지 수행하지 않고 중지해 수행 시간을 개선할 수 있음(n_estimators 200, 조기 중단 파라미터 값을 50으로 하면, 100회까지 학습 오류 값이 0.8인데 101~150회 반복하는 동안 예측 오류가 0.8보다 작은 값이 하나도 없으면 부스팅을 종료함)

---

 reference

파이썬 머신러닝 완벽 가이드
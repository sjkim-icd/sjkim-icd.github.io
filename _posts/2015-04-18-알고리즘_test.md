---
title:  "[알고리즘]"
excerpt: "md 파일에 마크다운 문법으로 작성하여 Github 원격 저장소에 업로드 해보자. 에디터는 Visual Studio code 사용! 로컬 서버에서 확인도 해보자. "

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]

toc: true
toc_sticky: true
 
date: 2020-05-25
last_modified_at: 2020-05-25

---


TEST

ESTTEST

1. 선형모델

- SVM과 로지스틱 회귀분석이
  선형모델계열이다.

(1) 장점

- 학습속도 및 예측속도 빠름
- 방대 및 희소 데이터에도 잘 작동
- 계수를 통한 수식으로 변수의 영향도를 쉽게 파악할 수 있다.
  (2) 단점
- 변수 간의 상관성이 높은 경우, 
  계수 값이 불명확한 경우가 생긴다.
- 샘플에 비해 특성이 많을 때 잘 작동한다.

[REGRESSION MODEL 상세] 
regression 모델에서 설명변수간 상관성이 높거나 미지수가 더 많은 경우 베타가 무한대로 가는데
Ridge와 LAsso를 통해 상수를 추가해서 bias를 주고, 이를 통해 variance는 줄여줍니다.
베타 = 1/(.000000 + bias 0.5) bias를 주기전엔 무한대로 가는 베타에
bias로 0.5를 줘서 variance를 줄여줍니다. 여기서 람다가 0이면 기존 regression입니다.

Ridge : 릿지 회귀식은 
잔차제곱합(RSS)+패널티(베타값)으로 이뤄져있습니다.

패널티는 파라미터의 제곱을 더한 것입니다.
람다가 클수록 리지회귀의 계수 추정치는 0에
가까워집니다.

상관성을 가지는 변수에 대해
적절한 가중치를 배분하는 효과가 있습니다.

LASSO: 라소회귀식은
패널티항에 절대값의 합을 줍니다.
유의미하지 않은 계수를 0에 가깝게 추정하여
변수선택효과를 줍니다.

LASSO와 RIDGE의 차이:
LASSO는 변수선택을 자동으로,
RIDGE는 선택이 아닌 가중치

과녁으로 비유하자면
전반적으로 있던 것이
LASSO가 bias를 통해 밀도를 높게 만들어줌(즉 
VARIANCE를 높게 만듦)


* 변수 중요도 확인

- Y와 X를 단변량으로 돌린 후 
  pvalue를 저장하고
  그래프를 보면서 , X와 Y의 관계가 선형, X^2, LOG 관계인지 확인

- 단변량 분석도, X와 Y의 관계가 선형일때는 의미가 있지만, 비선형인 경우 corr이 낮게 나와서 그런 변수를 빼먹을 수 있으므로, vaiable selection을 할때도 많이 쓰는게 랜덤포레스트, 이 방식은 비선형 방식을 고려할 수 있습니다. 랜덤포레스트도 단변량 분석을 진행하여 변수 중요도를 살펴볼 수 있겠죠.

2. 나이브 베이즈 분류
   (1) 장점

- 선형분류기보다 훈련/예측속도가 빠르다
- 고차원 데이터에서 잘 작동하며, 비교적 매개변수에 민감하지 않은 편이다.
  (2) 단점
  일반화 성능이 조금 뒤쳐진다.

3. 결정트리
   (1) 개념: 

- 규칙을 순차적으로 적용하면서 독립변수 공간을 분할하는 분류모형
- 분류와 예측을 수행
- 만들어진 모델을 쉽게 시각할 수 있음
- 각 변수의 영역을 반복적으로 분할하여 전체 영역에서의 규칙 생성
  즉 변수를 쪼개고 나누면서 규칙을 생성하는데
  LEAF 노드가 PURE해지면 더 이상 SPLIT하지 않습니다.
- 의사결정나무는 비선형 데이터를 잘 커버해줍니다.
- 규칙이 IF THEN 형식으로 이해하기 쉽고 해석하기 좋습니다.
- 모형에 대한 가정(등분산,선형)이 필요없는 비모수 
- 주요하게 사용하는 PARAMETER는 MAX_DEPTH(얼마나 분기할것인가), SAMPLE_RATE(데이터에 많이 의존하므SAMPLE조정), COL_SAMPLE_RATE(변수선택), MIN_ROWS(도착지점의 LEAFNODE 최소숫자)
  (2) 작동원리
  1] 어떤 변수부터 탐색하는가: 전체 변수가 아니라 10%/20%만 탐색하라는 식으로 진행됩니다. 변수1보다 변수2를 쪼갤때 leaf node가 pure해져서 불순도가 적다면 변수2를 자를 것입니다. 독립변수와 가능 기준값에 대한 정보획득량을 구하여 정보획득량이 가장 큰 독립변수와 기준값을 선택한다.

2] 연속형 변수일 경우 분할기준이 무엇인가: 전통적인 의사결정나무는 모든 범위를 잘라보고 불순도가 낮은 것을 선택하는데, h2o의 경우는 연속형변수를 10구간 정도 잘라서 불순도를 비교해보는 식으로 진행하여 속도가 빠름
3] TREE의 DEPTH를 어떻게 지정할 것인가:
DEPTH는 3/5/7 정도하고 더 내려가는 경우가 많지는 않다고 하는데
이건 데이터 바이 데이터인 듯합니다. 
4]  LEAF NODE 개수: LEAF NODE에 최소한 이정도는 있어라고 지정할 수 있는데 1개로 지정할 경우 OVERFITTING이 될 가능성이 높습니다. 최소 갯수 늘리면 안정적인 MODEL이 될 수 있겠죠. 
데이터에 따라 다르겠지만 LEAF NODE를 최소 20/100이상 해줘야 한다고 합니다. 데이터의 특징에 따라 leaf node갯수 조정 필요합니다. 

5] SPLIT RULE :카이제곱통계량p value,지니계수, 엔트로피 지수.
분할은 자식마디가 부모마디보다 순수도가 증가할 때 분류된다.
분류규칙은 부모와 자식 노드 간의 엔트로피를 가장 낮게 만드는 독립변수와 기준값을 찾는 방법이다. 이를 정량화한 것이 정보획득량(information gain)이라고 부른다.독립변수를 선택하여 독립변수에 대한 기준값을 정하여 해당 값 보다 큰 데이터그룹과 작은 데이터그룹으로 나눈다. 노드에 속한 데이터 클래스의 비율을 구하여 노드의 조건부 확률분포를 구하는데
이 조건부확률분포를 이용하여 클래스를 예측한다.
6]  PRUNNING(가지치기):가치지기를 해서 모형의 복잡성을 줄여 안정성을 강화합니다. 지나치게 많은 마디는 새로운 자료에 적용할 경우 예측오차가 커질 수 있습니다. 예전엔 prunning을 했는데요, CART로 해서 카이스퀘어 등으로 진행했는데 최근에는 maxdepth를 정해서 진행하므로 prunning과정을 거치지 않는 경우가 있다고 합니다.
7] 변수중요도: split의 불순도 감소량을 summary해서 주는 것 incNodePurity를 summary한 것, IncMSE(leaf노드의 오차가 줄어드는거)

의사결정나무는 정확도가 너무 떨어지는데요. 그래서 단일 모델로는 사용하지 않고 여러개를 만들어서 사용하거나 설명 할때 사용합니다.

(3) 종류
1) ID3
2) C.4.5/C.5.0

- 각 마디에서 다지 분리 가능
- 불순도 측도: 엔트로피지수
  4) CART
- 가장 널리 이용
- 범주형: 지니지수
- 연속형: 분산을 이용한 이진분리
  6) CHAID
- 가지치지 않고 적당한 크기에서 성장 중지
- 입력변수는 반드시 범주형
- 불순도 측도: 카이제곱 

1)~3)은 기계학습 분야에서 개발/엔트로피, 정보획득량 개념 활용
4)~5)는 통계학 기초/카이스퀘어, T,F검정의 통계분석법 활용
(1) 장점

- if then 형식으로 룰을 이해하기 쉽다.
- 연속형, 범주형 변수 모두 활용 가능
- 변수의 중요도를 산출하여 비교 가능
- 비교적 속도가 빠름

(2) 단점

- 여러 변수를 동시에 고려하지 않고
  한 변수를 선택하여 분류하기 때문에
  단일 변수에서 차이를 구분하지 못할때 분류율이 떨어짐
- greedy algorithm(문제 해결과정에서
  순간마다 최적이라고 생각되는 결정 방식을 택하여 최종 해답에 도달하는 문제해결 방식/계산속도가 큰 장점), Hill climbing(최적의 해를 찾아 값이 증가 또는 감소 방향으로 계속 이동하는 알고리즘)을 사용하는데
  이 방식은 최적의 해를 보장하지는 않음
- 연속형 변수값 예측시 적당하지 않음
- 트리 모형이 복잡하면 예측력이 저하되고
  해석 또한 어려워짐
- 데이터 변형에 민감, 레코드 개수의 차이에 따라 트리가 많이 달라질 수 있다. 비슷한 수준의 정보량을 가지는 두 변수가 있는데, 이 약간의 차이로 다른 변수가 선택될 경우, 트리 구성이 달라질 수 있다.

4. 앙상블
   주어진 자료로부터 여러 개의 예측모형을 만든 후 이를 결합하여 하나의 최종 예측모형을 만드는 방법

- 대표적인 앙상블 방법 : VOTING, BAGGING, BOOSTING + STACKING
  BAGGING, BOOSTING: 결정 트리 알고리즘 기반
  VOTING, STACKING: 서로 다른 알고리즘 기반

(1) BAGGING은 각 분류기가 같은 유형의 알고리즘이며,
데이터 샘플링을 서로 다르게 구성해  수행

(2) VOTING: 다른 알고리즘이 같은 데이터 셋을 학습하여
최종 예측 결과

(3) BOOSTING: 여러 개 분류기가 앞의 WEAK LEARNER를
보완하여 가중치 부여해서 순차적으로 학습

(4) STACKING: 여러 다른 모델의 예측 결과값을
학습데이터로 만들어 다른 모델로 재학습하여 결과를 예측하는 방법

4. 랜덤포레스트

의사결정나무는 분산이 큰 모델입니다.
불안정하다는건 데이터 포인트 몇개가 흔들리면 많이 변하는 걸 의미합니다. 불안정하지 않은 Robust한 모델을 만들기 위해 사용합니다.

배깅, 부스팅보다 더 많은 무작위성을 주어
다수의 붓스트랩 샘플(반복허용/복원추출)로
약한 학습기를 생성한 후 선형결합하여 최종 학습기를 만드는 방법인데요. 입력변수에 대해서도 무작위 추출합니다.
해석은 어렵지만 예측력은 높습니다.

- 작동원리
  (1) TRAIN DATA 붓스트랩으로 표본 생성
  (2) 입력변수 무작위 뽑아 의사결정나무 생성
  (3) 의사결정나무 선형 결합하여 최종 학습기 생성


5. GBM(GRADIENT BOOSTING)

- 작동원리
  (1) WEAK LEARNER의 양상을 보고, error에 따라 resampling하여 오분류 관측치에 가중치를 부여하고 보다 더 잘 예측하도록 다음 학습기 생성
  (2) 다수결을 매길 때도 weighted avg나 weighted vote를 함(주주총회st)

- 특징
  그래디언트 부스팅은 너무 많이 반복할 경우 과대적합의 문제가 발생합니다.
  반복수를 조절하여 최적 예측력 모델을 찾아야 합니다.

RANDOM FOREST는 parallel
GBM는 sequntial

 의사결정나무관련http://blog.naver.com/trashx/60099037740
https://muzukphysics.tistory.com/138

전체 비교
https://dohk.tistory.com/170?fbclid=IwAR0GK5uZ6iTh0T2k1Mpz0e1MB9FpGo8EnxAYR9mY5vTDhhT010lCvK0Cg_k

https://wikidocs.net/
https://datascienceschool.net/03%20machine%20learning/12.01%20%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4.html
알고리즘 관련해서 강의가 잇는지
edwith



99. overfitting/underfitting 해결방법

- overfitting 과적합의 의미: 모델이 TRAIN SET에 과도하게 FITTING되어 TRAIN은 높은 정확도가 산출되지만
  TEST SET에 대해서는 낮은 정확도를 보이는 현상입니다.

오버피팅은 high variance로
세부적인 특성까지 반영되어
학습된 특성은 예측을 잘하지만
학습되지 않은 특성의 경우, 예측을 잘하지 못합니다.
---
title:  "[머신러닝] 배깅과 부스팅"
excerpt: "배깅과 부스팅 "

categories:
  - machinelearning
tags:
  - [machinelearning, bagging, 배깅, 부스팅, 앙상블]

toc: true
toc_sticky: true
 
date: 2021-10-01
last_modified_at: 2021-10-01

---




# 배깅과 부스팅

**[배깅과 부스팅 기본 개념]**

- 배깅, 부스팅 모두 머신러닝에서 앙상블 학습 방법
- 앙상블 학습은 여러 모델을 결합하여 기계 학습의 모델 성능 향상 시킴
- 앙상블 기본 원리: 각각 학습하여 분류기를 만들고 각 분류기가 투표를 하는 개념, 이러한 과정을 통해 단일 모델에 비해 성능을 향상시킴
- 앙상블의 장점: 서로 다른 모형의 여러 추정치를 결합하여 단일 추정치의 분산을 감소시키고, 이에 따라 안정성이 더 높은 모형을 만들 수 있음, 즉 학습에서 error를 유발하는 noise, bias, variance를 최소화하는데 도움이 됨
- 배깅은 모형의 variance를 줄이고, 부스팅은 모델의 bias를 줄이는 데 도움이 됨

![https://quantdare.com/wp-content/uploads/2016/04/bb1-800x221.png](https://quantdare.com/wp-content/uploads/2016/04/bb1-800x221.png)

1. **Bootstrapping**
- **Bootstrapping** 개념: 데이터셋에서 작은 집합에 대해 무작위 샘플링을 진행

![https://miro.medium.com/max/1010/1*YYom-NKDaZ-B7RB_891DgQ.png](https://miro.medium.com/max/1010/1*YYom-NKDaZ-B7RB_891DgQ.png)

1. 배깅
- 배깅 개념: 여러 모델의 결과를 결합, 일반적으로 분산이 높은 머신러닝 알고리즘인 의사결정나무에 주로 적용함

![https://miro.medium.com/max/978/0*g7yNQvSRuqEgjj25.png](https://miro.medium.com/max/978/0*g7yNQvSRuqEgjj25.png)

- 배깅 적용 방법
1) 원 데이터 집합에서 여러 하위 집합을 생성
2) 각 하위 집합에 대해 weak model(base model)을 생성
3) 모형은 병렬적으로 실행되며, 서로 독립적임
4) 최종 예측은 모든 모형의 예측을 결합하여 결정 됨

![https://miro.medium.com/max/1012/0*mA5rdZipfga2HLjO.png](https://miro.medium.com/max/1012/0*mA5rdZipfga2HLjO.png)

1. **부스팅**
- 부스팅 개념: 모델링의 과정은 순차적으로 진행되며, 이전 모델의 오류를 수정하려고 후속모델이 진행됨, 즉 랜덤 표본을 적합시키고 이전 모델의 오차를 해결해가는 방법, 잘못 분류되면 해당 부분에서의 입력 가중치가 증가하여 다음 모델에서는 올바르게 분류할 가능성을 높임, 전체 세트를 결합함으로써 weak model을 더 나은 수행 모델로 변환함
- 부스팅 적용 방법:
1) 원 데이터 집합에서 여러 하위 집합을 생성
2) 처음에는 모든 관측치에 동일한 가중치가 부여됨
3) 하위 집합에 대해 weak model(base model)을 생성되고 이모델은 전체 데이터 집합을 예측하는데 사용됨
3) 잘못 예측된 관측치에 대해 더 높은 가중치가 부여되고 이에 따라 다음 모델을 생성함
4) 각각 이전 모델의 오류를 수정하며 여러 차례 반복함
5) 최종 모델(strong learner)는 모든 모델(weak learners)의 가중 평균

![https://miro.medium.com/max/1202/1*k-HYpwcgzCq_Yy--05_LAw.png](https://miro.medium.com/max/1202/1*k-HYpwcgzCq_Yy--05_LAw.png)

1. 배깅과 부스팅 중 어떤 것을 선택할까
- 배깅과 부스팅 모두 서로 다른 모형의 여러 추정치를 결합하여 단일 추정치의 분산을 줄이며 이를 통해 안정성이 높은 모형을 만듦
- 어떤 것을 선택할까라는 문제는 데이터, 시뮬레이션, 상황에 따라 다름
- 단일 모델의 성능이 매우 떨어진다면 배깅은 더 나은 bias를 얻지 못하는 반면에 부스팅은 단일 모델의 장점을 최적화하고 pirfalls를 줄이기 때문에 오류가 적은 결합 모델을 생성할 수 잇음
- 반대로 단일 모델이 과적합인 경우에 배깅이 더 낫다고 볼 수 있음, 부스팅은 과적합을 피하는데 도움이 되지 않음
1. 배깅과 부스팅의 공통점과 차이점
- 공통점: 둘 다 N개의 learner를 얻는 앙상블 방법임, 둘다 랜덤샘플링을 통해 훈련 데이터세트를 생성하고 weak learner의 평균을 내려 최종 결정을 내림, 이로 인해 분산을 줄이고 안정성이 높음
- 차이점: 
1) 배깅은 동일 유형에 속하는 예측을 결합하는 방법, 부스팅은 다른 유형에 속하는 예측을 결합하는 방법
2) 배깅은 bias가 아닌 vairance를 줄이는 걸 목표로 하고, 부스팅은 variance가 아닌 bias를 줄이는 것을 목표로 함
3) 배깅에서 각 모델은 동일한 가중치를 받지만 부스팅에서는 성능에 따라 가중치를 부여 받음
4) 배깅의 각 모델은 독립적으로 만들어지지만, 부스팅에서는 새로운 모델은 이전에 제작된 모델의 성능에 영향을 받음
1. 결론 및 요약
모델이 분산이 높아 불안정하면 배깅을 적용하고, 안정적이고 편향이 높아 단순하면 부스팅을 적용해야 함